#!/usr/bin/env bash

set -euo pipefail

# Vars
SCRIPTS_DIR="$(dirname "${BASH_SOURCE[0]}")"
. "${SCRIPTS_DIR}/../bash-lib/init"
set -x # After bash-lib init
TEST_VAR="TestPolicy/TestVariable"
TEST_VAR_VALUE_1="$(uuidgen)"
TEST_VAR_VALUE_2="$(uuidgen)"
BASE_URL="https://${INFRAPOOL_STACK_NAME}.${DOMAIN_NAME}"
AB_REQUESTS="500"

# Relative because it will be used within docker
TEST_POLICY_FILE="scripts/test_policy.yml"

collect_lb_logs(){
  echo "Collecting LB Logs"
  # LB logs are delievered every 5 minutes
  echo "Sleeping 6 minutes to allow async log delivery to complete"
  sleep 360
  mkdir -p lblogsync lb_logs
  # Download all the LB logs from s3 to lblogsync
  aws s3 sync "s3://org-conjur-${INFRAPOOL_STACK_NAME}-alb-log-bucket" lblogsync
  # Move all the logs to lb_logs to flatten the hierachy
  find lblogsync -name "*.log.gz" | while read -r log; do
    mv "${log}" lb_logs
    # Unzip because jenkins doesn't unzip artifacts in the view route
    # (Jenkins does unzip job logs, but not artifacts)
    # One at a time as sometimes there is an invalid zip file :(
    gunzip "lb_logs/$(basename "${log}")" ||:
  done
  rm -rf lblogsync
}


# Main flow

export LOG_GROUP="${INFRAPOOL_STACK_NAME}-conjur-log-group"
"${SCRIPTS_DIR}"/tail_ecs_logs &
LOG_TAIL_PID="${!}"
trap 'collect_lb_logs; kill -9 ${LOG_TAIL_PID} || true' EXIT

# Exercise is often run first thing after deploy. Ensure the cluster has settled
# before getting started.
for _ in {1..120}; do
  if curl -k "${BASE_URL}" | grep -i "your conjur server is running"; then
    echo "Exercise script detects conjur is up, proceeding to exercise."
    break
  else
    echo "Exercise script waiting for conjur to come up"
    sleep 5
  fi
done;

. "${SCRIPTS_DIR}/conjur_login"

# Load Policy
if dconjur list variables |grep -q "${TEST_VAR}"; then
  echo "Policy already loaded, skipping policy load"
else
 dconjur policy load root "${TEST_POLICY_FILE}"
  echo "Test Policy Loaded"
fi

# Ensure Variable defined by policy is available
if dconjur list variables |grep "${TEST_VAR}"; then
  echo "Verified that variable ${TEST_VAR} exists"
else
  bl_die "${TEST_VAR} not found after policy load"
fi

# Set variable value and read it back
dconjur variable values add "${TEST_VAR}" "${TEST_VAR_VALUE_1}"
check_variable_value "${TEST_VAR}" "${TEST_VAR_VALUE_1}"

# Update vairable to a new value and read it back
dconjur variable values add "${TEST_VAR}" "${TEST_VAR_VALUE_2}"
check_variable_value "${TEST_VAR}" "${TEST_VAR_VALUE_2}"

echo "Starting Reliability Tests"

# kill the log tailer before reliability testing to avoid hitting
# log api rate limits
kill "${LOG_TAIL_PID}" || true
echo "Checking that ECS log tailer process has stopped:"
ps "${LOG_TAIL_PID}" || true

# Build apache bench container
docker build -f "${SCRIPTS_DIR}/Dockerfile.ab" -t ab "${SCRIPTS_DIR}"
ab(){
  concurrency="${1}"
  name="${2}"
  shift; shift
  log="ab_${concurrency}_${name}.log"
  echo "Recording ab results to ${log}"
  docker run\
    --rm \
    --volume "${PWD}:${PWD}" \
    --workdir "${PWD}" \
    ab \
    -l \
    -n "${AB_REQUESTS}" \
    -c "${concurrency}" \
    -v 4 \
    "${@}" \
    &> "${log}"
  grep '^Failed requests:\s*0' "${log}"
  ! grep '^Non-2xx responses:' "${log}"
}

echo "Testing serial requests to root url"
ab 1 root "${BASE_URL}/"

echo "Testing parallel requests to root url"
ab 50 root "${BASE_URL}/"

echo "Testing serial variable reads"
ab 1 "read" -H "$(dconjur authn authenticate -H)" "${BASE_URL}/secrets/conjur/variable/${TEST_VAR}"

echo "Testing parallel variable reads"
ab 50 "read" -H "$(dconjur authn authenticate -H)" "${BASE_URL}/secrets/conjur/variable/${TEST_VAR}"

echo "Testing serial write"
date > postfile
ab 1 write -H "$(dconjur authn authenticate -H)" -p postfile "${BASE_URL}/secrets/conjur/variable/${TEST_VAR}"

echo "Waiting for cluster to settle after reliability tests"
until dconjur list variables; do
  echo "Waiting for cluster to settle after reliability tests..."
done
echo "Cluster has settled"

echo "Exercise Complete"
